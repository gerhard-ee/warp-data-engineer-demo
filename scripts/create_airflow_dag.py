"""
Snowflake ETL Data Pipeline
A sample Airflow DAG that demonstrates a data engineering workflow with Snowflake.
This DAG extracts data from a source, loads it to Snowflake, and performs transformations.

# --- Warp AI Prompt Demo Examples ---
# To create this file, a data engineer could use these AI prompts in Warp:
#
# 1. "Generate a basic Airflow DAG structure for a Snowflake ETL workflow"
# 2. "How do I configure the Snowflake connection in an Airflow DAG?"
# 3. "Write a Python function to generate sample sales data for my Airflow DAG"
# 4. "Show me how to load data from S3 to Snowflake using Airflow operators"
# 5. "Create a Snowflake SQL query that transforms sales data into a summary table"
# 6. "How do I implement data quality checks in Snowflake using Airflow?"
"""

from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.providers.snowflake.operators.snowflake import SnowflakeOperator
from airflow.providers.snowflake.transfers.s3_to_snowflake import S3ToSnowflakeOperator
from airflow.providers.amazon.aws.operators.s3 import S3CreateObjectOperator
import pandas as pd
import io
import json
import logging

# --- Warp AI Prompt Demo: "What Airflow imports do I need for a Snowflake ETL pipeline?" ---
# The above imports could be generated by asking Warp AI for the necessary imports
# Warp would provide the exact import statements needed, saving time looking up documentation

# DAG default arguments
default_args = {
    'owner': 'data_engineer',
    'depends_on_past': False,
    'email_on_failure': True,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
    'start_date': datetime(2023, 1, 1),
}

# --- Warp AI Prompt Demo: "How do I set up Snowflake connection parameters in Airflow?" ---
# A data engineer could ask Warp AI how to properly configure Snowflake connections
# Warp would provide the following configuration template
SNOWFLAKE_CONN_ID = 'snowflake_default'
SNOWFLAKE_SCHEMA = 'PUBLIC'
SNOWFLAKE_WAREHOUSE = 'COMPUTE_WH'
SNOWFLAKE_DATABASE = 'DEMO_DB'
SNOWFLAKE_ROLE = 'ACCOUNTADMIN'
S3_BUCKET = 'example-data-bucket'
S3_KEY = 'data/sales_{{ ds }}.csv'
SNOWFLAKE_STAGE = 'DEMO_STAGE'
SNOWFLAKE_TABLE = 'SALES'

# --- Warp AI Prompt Demo: "Generate an Airflow DAG definition for daily Snowflake ETL" ---
with DAG(
    'snowflake_etl_pipeline',
    default_args=default_args,
    description='ETL pipeline for loading and transforming data in Snowflake',
    schedule_interval=timedelta(days=1),
    catchup=False,
    tags=['snowflake', 'etl', 'data_engineering'],
) as dag:
    
    # --- Warp AI Prompt Demo: "Write a Python function to generate sample sales data" ---
    # When unsure how to generate test data, Warp AI can provide a complete function implementation
    def generate_sample_data(**kwargs):
        """Generate sample sales data for demonstration purposes."""
        logging.info("Generating sample sales data")
        
        # Create sample sales data
        data = {
            'order_id': range(1000, 1100),
            'customer_id': range(1, 101),
            'product_id': range(500, 600),
            'quantity': [5, 10, 15, 20, 25] * 20,
            'amount': [round(i * 10.5, 2) for i in range(1, 101)],
            'order_date': [kwargs['ds']] * 100  # Use execution date
        }
        
        df = pd.DataFrame(data)
        
        # Convert to CSV
        csv_buffer = io.StringIO()
        df.to_csv(csv_buffer, index=False)
        
        return csv_buffer.getvalue()
    
    # --- Warp AI Prompt Demo: "How do I create a PythonOperator in Airflow?" ---
    extract_data = PythonOperator(
        task_id='extract_sample_data',
        python_callable=generate_sample_data,
        provide_context=True,
        dag=dag,
    )
    
    # --- Warp AI Prompt Demo: "Show me how to upload data to S3 using Airflow" ---
    # When facing new operators, asking Warp AI provides correct implementation examples
    upload_to_s3 = S3CreateObjectOperator(
        task_id='upload_to_s3',
        aws_conn_id='aws_default',
        bucket_name=S3_BUCKET,
        key=S3_KEY,
        data="{{ task_instance.xcom_pull(task_ids='extract_sample_data') }}",
        replace=True,
        dag=dag,
    )
    
    # --- Warp AI Prompt Demo: "Generate Snowflake SQL to create a sales table" ---
    # SQL creation can be simplified by asking Warp AI for specific schemas
    create_table = SnowflakeOperator(
        task_id='create_snowflake_table',
        snowflake_conn_id=SNOWFLAKE_CONN_ID,
        sql=f"""
        CREATE TABLE IF NOT EXISTS {SNOWFLAKE_TABLE} (
            order_id INTEGER,
            customer_id INTEGER,
            product_id INTEGER,
            quantity INTEGER,
            amount FLOAT,
            order_date DATE
        )
        """,
        warehouse=SNOWFLAKE_WAREHOUSE,
        database=SNOWFLAKE_DATABASE,
        schema=SNOWFLAKE_SCHEMA,
        role=SNOWFLAKE_ROLE,
        dag=dag,
    )
    
    # --- Warp AI Prompt Demo: "How do I create a Snowflake stage for S3 in Airflow?" ---
    # Complex SQL with placeholders can be generated through Warp AI prompts
    create_stage = SnowflakeOperator(
        task_id='create_snowflake_stage',
        snowflake_conn_id=SNOWFLAKE_CONN_ID,
        sql=f"""
        CREATE OR REPLACE STAGE {SNOWFLAKE_STAGE}
        URL='s3://{S3_BUCKET}'
        CREDENTIALS=(AWS_KEY_ID='{{{{ conn.aws_default.login }}}}' AWS_SECRET_KEY='{{{{ conn.aws_default.password }}}}')
        FILE_FORMAT=(TYPE='CSV' SKIP_HEADER=1);
        """,
        warehouse=SNOWFLAKE_WAREHOUSE,
        database=SNOWFLAKE_DATABASE,
        schema=SNOWFLAKE_SCHEMA,
        role=SNOWFLAKE_ROLE,
        dag=dag,
    )
    
    # --- Warp AI Prompt Demo: "What's the correct syntax for S3ToSnowflakeOperator?" ---
    # Warp AI can explain proper parameter usage for complex operators
    load_to_snowflake = S3ToSnowflakeOperator(
        task_id='load_to_snowflake',
        s3_keys=[S3_KEY],
        table=SNOWFLAKE_TABLE,
        schema=SNOWFLAKE_SCHEMA,
        stage=SNOWFLAKE_STAGE,
        file_format="(type='CSV', skip_header=1)",
        snowflake_conn_id=SNOWFLAKE_CONN_ID,
        dag=dag,
    )
    
    # --- Warp AI Prompt Demo: "Write a Snowflake SQL query to create a summary table from sales data" ---
    # Complex transformations can be generated by asking Warp AI for specific business logic
    transform_data = SnowflakeOperator(
        task_id='transform_data',
        snowflake_conn_id=SNOWFLAKE_CONN_ID,
        sql=f"""
        -- Create a summary table of sales by date
        CREATE OR REPLACE TABLE SALES_SUMMARY AS
        SELECT 
            order_date,
            COUNT(DISTINCT order_id) AS total_orders,
            COUNT(DISTINCT customer_id) AS unique_customers,
            SUM(quantity) AS total_items_sold,
            SUM(amount) AS total_revenue,
            AVG(amount) AS average_order_value
        FROM {SNOWFLAKE_TABLE}
        GROUP BY order_date;
        """,
        warehouse=SNOWFLAKE_WAREHOUSE,
        database=SNOWFLAKE_DATABASE,
        schema=SNOWFLAKE_SCHEMA,
        role=SNOWFLAKE_ROLE,
        dag=dag,
    )
    
    # --- Warp AI Prompt Demo: "How can I implement data quality checks in Snowflake using Airflow?" ---
    # Data quality patterns can be quickly implemented with Warp AI assistance
    validate_data = SnowflakeOperator(
        task_id='validate_data',
        snowflake_conn_id=SNOWFLAKE_CONN_ID,
        sql=f"""
        -- Simple data quality checks
        SELECT 
            CASE 
                WHEN COUNT(*) < 10 THEN RAISE_ERROR('Too few records loaded')
                WHEN COUNT(*) > 1000 THEN RAISE_ERROR('Too many records loaded')
                ELSE 'Data validation passed'
            END AS validation_status
        FROM {SNOWFLAKE_TABLE}
        WHERE order_date = '{{{{ ds }}}}';
        """,
        warehouse=SNOWFLAKE_WAREHOUSE,
        database=SNOWFLAKE_DATABASE,
        schema=SNOWFLAKE_SCHEMA,
        role=SNOWFLAKE_ROLE,
        do_xcom_push=True,
        dag=dag,
    )
    
    # --- Warp AI Prompt Demo: "Write a function to export Snowflake data to JSON" ---
    # Complex integrations can be implemented by asking Warp AI for end-to-end solutions
    def export_summary(**kwargs):
        """Export the summary data from Snowflake to JSON format."""
        import snowflake.connector
        from airflow.hooks.base import BaseHook
        
        # Get Snowflake connection details
        conn = BaseHook.get_connection(SNOWFLAKE_CONN_ID)
        snowflake_conn = snowflake.connector.connect(
            user=conn.login,
            password=conn.password,
            account=conn.extra_dejson.get('account'),
            warehouse=SNOWFLAKE_WAREHOUSE,
            database=SNOWFLAKE_DATABASE,
            schema=SNOWFLAKE_SCHEMA,
            role=SNOWFLAKE_ROLE
        )
        
        # Execute query
        cursor = snowflake_conn.cursor()
        cursor.execute(f"SELECT * FROM SALES_SUMMARY WHERE order_date = '{kwargs['ds']}'")
        
        # Fetch results
        columns = [col[0] for col in cursor.description]
        data = cursor.fetchall()
        
        # Convert to JSON
        result = []
        for row in data:
            result.append(dict(zip(columns, row)))
        
        with open(f"/tmp/sales_summary_{kwargs['ds']}.json", "w") as f:
            json.dump(result, f, default=str)
            
        return f"Summary exported to /tmp/sales_summary_{kwargs['ds']}.json"
    
    # --- Warp AI Prompt Demo: "How do I create a PythonOperator that exports data?" ---
    export_summary_task = PythonOperator(
        task_id='export_summary',
        python_callable=export_summary,
        provide_context=True,
        dag=dag,
    )
    
    # --- Warp AI Prompt Demo: "How do I set up Airflow task dependencies?" ---
    # Quickly learn task dependency syntax with Warp AI
    extract_data >> upload_to_s3 >> [create_table, create_stage] >> load_to_snowflake >> transform_data >> validate_data >> export_summary_task

    # --- Warp AI Prompt Demo: "How do I add logging to my Airflow DAG?" ---
    # Proper logging practices can be learned through Warp AI
    def log_completion(**kwargs):
        """Log completion of the DAG run."""
        logging.info(f"ETL pipeline completed successfully for {kwargs['ds']}")
        logging.info("Pipeline execution metrics:")
        logging.info(f"- Execution date: {kwargs['ds']}")
        logging.info(f"- DAG run ID: {kwargs['run_id']}")
        
        # Demo how Warp AI could help debug issues
        logging.info("If you encounter errors, you could ask Warp AI:")
        logging.info("'Debug this Airflow task failure: [paste error]'")
        logging.info("'How do I fix Snowflake connection issues in Airflow?'")
        logging.info("'Optimize this Snowflake query for better performance'")
        
    # --- Warp AI Prompt Demo: "Add a final task to my Airflow DAG" ---
    log_task = PythonOperator(
        task_id='log_completion',
        python_callable=log_completion,
        provide_context=True,
        dag=dag,
    )
    
    export_summary_task >> log_task

# --- Warp AI Prompt Demo: "What are some common Airflow DAG troubleshooting tips?" ---
# At the end of the file, a data engineer might add notes based on Warp AI advice:
"""
Troubleshooting Tips (Generated by Warp AI):
1. Check Airflow logs with: airflow tasks test snowflake_etl_pipeline [task_id] [execution_date]
2. Validate Snowflake connections: airflow connections get snowflake_default
3. For S3 permission issues, verify AWS credentials in Airflow connections
4. Remember to set up Snowflake warehouse auto-suspend to manage costs
5. Use Airflow variables for environment-specific configurations

For more information on Airflow best practices with Snowflake, ask Warp AI:
"What are the best practices for Airflow DAGs with Snowflake?"
"""

